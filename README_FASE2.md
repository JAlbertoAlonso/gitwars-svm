# ‚öôÔ∏è Fase 2 ‚Äî Entrenamiento lineal (SGD/Subgradiente)

## üéØ Objetivo
Entrenar un **SVM lineal** usando **subgradientes** de la *hinge loss*.

---

## üß© Instrucciones

Crea un nuevo archivo:

```
fase2_equipo.py
```

Implementa el entrenamiento basado en el siguiente esquema:

```python
# Pseudoc√≥digo gu√≠a
Inputs: X, y in {-1, +1}, C, eta (lr), epochs
Init: w = 0, b = 0

for epoch in [1..epochs]:
    shuffle data
    for (x_i, y_i) in minibatches:
        margin_i = y_i * (w¬∑x_i + b)
        if margin_i >= 1:
            grad_w = w
            grad_b = 0
        else:
            grad_w = w - C * y_i * x_i
            grad_b = - C * y_i
        w = w - eta * grad_w
        b = b - eta * grad_b

    # Calcular p√©rdida por √©poca
    hinge_loss = promedio( max(0, 1 - y * (X¬∑w + b)) )
    L = L * hinge_loss
    registrar(L)

return w, b
```

---

## ‚úÖ Criterios de aceptaci√≥n

- Mostrar una **curva decreciente** de la funci√≥n de costo total $L$ a lo largo de las √©pocas.
- Calcular $L = \frac{1}{2}\|w\|^2 + C \cdot \text{hinge\_loss}$ en cada iteraci√≥n.
- Reportar **accuracy final** sobre `train_linear.csv`.
- Incluir una **gr√°fica de la frontera de decisi√≥n** $w^T x + b = 0$.

---

## üß† Preguntas gu√≠a para el PR

1. ¬øC√≥mo afecta $C$ la cantidad de violaciones al margen?  
2. ¬øQu√© ocurre si la tasa de aprendizaje es muy grande o muy peque√±a?

---

> üì§ **Entrega:**  
> Sube tu archivo `fase2_equipo.py` y tu gr√°fica mediante un Pull Request.
